{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34618,"status":"ok","timestamp":1729061085094,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"P5kTYvEvSXSm","outputId":"9bac1f82-3f91-4d75-eb3a-6668122ad674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.11.8 torchmetrics-1.4.3\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/FasterRCNNFiles')\n","\n","!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":532,"status":"ok","timestamp":1726824801221,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"LDX2R5JOou6f","outputId":"50af6669-759c-4a9f-9674-439d26aa11d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.4.1+cu121\n","0.19.1+cu121\n"]}],"source":["print(torch.__version__)\n","print(torchvision.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVDJ9ePBp45M"},"outputs":[],"source":["import csv\n","\n","# Define a CSV file to store the loss values\n","csv_file = '/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNData.csv'\n","\n","# Create or open the CSV file and write the header (only once)\n","with open(csv_file, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Epoch', 'Iteration', 'Train loss_classifier', 'Train loss_box_reg', 'Train loss_objectness', 'Train loss_rpn_box_reg', 'Train total_loss',\n","                     'Val loss_classifier', 'Val loss_box_reg', 'Val loss_objectness', 'Val loss_rpn_box_reg', 'Val total_loss',\n","                     'Training mAP', 'Training mAP50', 'Training mAP75', 'Training AR1', 'Training AR10', 'Training AR100',\n","                     'Training map_small', 'Training map_medium', 'Training map_large', 'Training mar_small', 'Training mar_medium', 'Training mar_large',\n","                     'Validation mAP', 'Validation mAP50', 'Validation mAP75', 'Validation AR1', 'Validation AR10', 'Validation AR100',\n","                     'Validation map_small', 'Validation map_medium', 'Validation map_large', 'Validation mar_small', 'Validation mar_medium', 'Validation mar_large',\n","                     'Learning Rate'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"elapsed":7407,"status":"error","timestamp":1728900352411,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"2naG2lXDA1LO","outputId":"ef4e3538-a034-4c8c-a939-056de5d3523d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'config'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-bd0bb96becd0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_bounding_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from utils import (\n\u001b[1;32m      8\u001b[0m     \u001b[0mget_model_instance_segmentation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["#verfying the dataset creation and annotations are correct\n","\n","import torchvision.transforms.functional as F\n","from albumentations.pytorch.transforms import ToTensorV2\n","from torchvision.utils import draw_bounding_boxes\n","import config\n","from utils import (\n","    get_model_instance_segmentation,\n","    collate_fn,\n","    get_transform,\n","    myOwnDataset,\n",")\n","\n","\n","#id2label = {1: \"nest\"}\n","dataset = myOwnDataset(\n","    root=config.train_data_dir, annotation=config.train_coco, transforms=get_transform()\n",")\n","img, target = dataset[1]\n","print(target)\n","#labels = [for id in target[\"labels\"].tolist()]\n","\n","#print(labels)\n","\n","img = draw_bounding_boxes(\n","    img, target[\"boxes\"], colors=\"Turquoise\", width=2\n",")\n","img = F.to_pil_image(img.detach())\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeRmXu1nDi8m"},"outputs":[],"source":["import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n","\n","class myOwnDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Own coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = coco.loadImgs(img_id)[0][\"file_name\"]\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, path))\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i][\"bbox\"][0]\n","            ymin = coco_annotation[i][\"bbox\"][1]\n","            xmax = xmin + coco_annotation[i][\"bbox\"][2]\n","            ymax = ymin + coco_annotation[i][\"bbox\"][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # Labels (In my case, I only one class: target class or background)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i][\"area\"])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        # Iscrowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","\n","# In my case, just added ToTensor\n","def get_transform():\n","    custom_transforms = [torchvision.transforms.ToTensor()]\n","    #custom_transforms.append(c)\n","    return torchvision.transforms.Compose(custom_transforms)\n","\n","\n","# collate_fn needs for batch\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    #model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1,min_size=640,max_size=640)\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8226567,"status":"ok","timestamp":1729069346372,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"o4ASMS0GEecP","outputId":"6603aba9-7df3-4e4a-93ec-155f82513095"},"outputs":[{"name":"stdout","output_type":"stream","text":["Torch version: 2.4.1+cu121\n","loading annotations into memory...\n","Done (t=4.59s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=1.60s)\n","creating index...\n","index created!\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:00<00:00, 224MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Model Structure:\n","FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(640,), max_size=640, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n","    )\n","  )\n",")\n","Epoch: 0/50\n","Training Metrics:\n","{'map': tensor(0.5376), 'map_50': tensor(1.), 'map_75': tensor(0.4347), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.5376), 'mar_1': tensor(0.3833), 'mar_10': tensor(0.5917), 'mar_100': tensor(0.5917), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.5917), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.21625177562236786\n","Validation on Validation Set:\n","\n","{'map': tensor(0.3841), 'map_50': tensor(0.8647), 'map_75': tensor(0.2663), 'map_small': tensor(0.3047), 'map_medium': tensor(0.5455), 'map_large': tensor(0.3666), 'mar_1': tensor(0.3169), 'mar_10': tensor(0.5366), 'mar_100': tensor(0.5415), 'mar_small': tensor(0.5000), 'mar_medium': tensor(0.6333), 'mar_large': tensor(0.5245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.21625177562236786\n","Epoch: 1/50\n","Training Metrics:\n","{'map': tensor(0.4143), 'map_50': tensor(0.9391), 'map_75': tensor(0.2310), 'map_small': tensor(-1.), 'map_medium': tensor(0.6109), 'map_large': tensor(0.3562), 'mar_1': tensor(0.3500), 'mar_10': tensor(0.5300), 'mar_100': tensor(0.5300), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.6333), 'mar_large': tensor(0.4857), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.16553044319152832\n","Validation on Validation Set:\n","\n","{'map': tensor(0.4470), 'map_50': tensor(0.9320), 'map_75': tensor(0.3637), 'map_small': tensor(0.4965), 'map_medium': tensor(0.5584), 'map_large': tensor(0.4251), 'mar_1': tensor(0.3514), 'mar_10': tensor(0.5768), 'mar_100': tensor(0.5768), 'mar_small': tensor(0.6000), 'mar_medium': tensor(0.6375), 'mar_large': tensor(0.5618), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.16553044319152832\n","Epoch: 2/50\n","Training Metrics:\n","{'map': tensor(0.6060), 'map_50': tensor(0.9010), 'map_75': tensor(0.6894), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.6060), 'mar_1': tensor(0.5000), 'mar_10': tensor(0.6200), 'mar_100': tensor(0.6200), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.6200), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.12137480825185776\n","Validation on Validation Set:\n","\n","{'map': tensor(0.4749), 'map_50': tensor(0.9436), 'map_75': tensor(0.3786), 'map_small': tensor(0.5350), 'map_medium': tensor(0.5937), 'map_large': tensor(0.4576), 'mar_1': tensor(0.3648), 'mar_10': tensor(0.6092), 'mar_100': tensor(0.6099), 'mar_small': tensor(0.6375), 'mar_medium': tensor(0.6708), 'mar_large': tensor(0.5945), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.12137480825185776\n","Epoch: 3/50\n","Training Metrics:\n","{'map': tensor(0.7550), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.7550), 'mar_1': tensor(0.6200), 'mar_10': tensor(0.8000), 'mar_100': tensor(0.8000), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.8000), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.12275400757789612\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5093), 'map_50': tensor(0.9545), 'map_75': tensor(0.4741), 'map_small': tensor(0.6090), 'map_medium': tensor(0.6291), 'map_large': tensor(0.4886), 'mar_1': tensor(0.3965), 'mar_10': tensor(0.6134), 'mar_100': tensor(0.6134), 'mar_small': tensor(0.7000), 'mar_medium': tensor(0.6750), 'mar_large': tensor(0.5936), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.12275400757789612\n","Epoch: 4/50\n","Training Metrics:\n","{'map': tensor(0.7600), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(0.7948), 'map_medium': tensor(0.7337), 'map_large': tensor(0.7635), 'mar_1': tensor(0.4846), 'mar_10': tensor(0.7846), 'mar_100': tensor(0.7846), 'mar_small': tensor(0.8000), 'mar_medium': tensor(0.7667), 'mar_large': tensor(0.7833), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.12137822806835175\n","Validation on Validation Set:\n","\n","{'map': tensor(0.4985), 'map_50': tensor(0.9398), 'map_75': tensor(0.4818), 'map_small': tensor(0.6395), 'map_medium': tensor(0.6202), 'map_large': tensor(0.4725), 'mar_1': tensor(0.3958), 'mar_10': tensor(0.6042), 'mar_100': tensor(0.6042), 'mar_small': tensor(0.7375), 'mar_medium': tensor(0.6750), 'mar_large': tensor(0.5791), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.12137822806835175\n","Epoch: 5/50\n","Training Metrics:\n","{'map': tensor(0.6904), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(0.7221), 'map_large': tensor(0.6859), 'mar_1': tensor(0.6000), 'mar_10': tensor(0.7500), 'mar_100': tensor(0.7500), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.7667), 'mar_large': tensor(0.7429), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.08155782520771027\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5000), 'map_50': tensor(0.9365), 'map_75': tensor(0.4882), 'map_small': tensor(0.6047), 'map_medium': tensor(0.6250), 'map_large': tensor(0.4734), 'mar_1': tensor(0.4014), 'mar_10': tensor(0.6035), 'mar_100': tensor(0.6035), 'mar_small': tensor(0.6750), 'mar_medium': tensor(0.6958), 'mar_large': tensor(0.5782), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.08155782520771027\n","Epoch: 6/50\n","Training Metrics:\n","{'map': tensor(0.8493), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.8493), 'mar_1': tensor(0.7667), 'mar_10': tensor(0.8556), 'mar_100': tensor(0.8556), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.8556), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.05890043079853058\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5223), 'map_50': tensor(0.9586), 'map_75': tensor(0.4937), 'map_small': tensor(0.6846), 'map_medium': tensor(0.6371), 'map_large': tensor(0.4977), 'mar_1': tensor(0.4134), 'mar_10': tensor(0.6204), 'mar_100': tensor(0.6204), 'mar_small': tensor(0.7125), 'mar_medium': tensor(0.7000), 'mar_large': tensor(0.5964), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.05890043079853058\n","Epoch: 7/50\n","Training Metrics:\n","{'map': tensor(0.8283), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.8283), 'mar_1': tensor(0.5583), 'mar_10': tensor(0.8500), 'mar_100': tensor(0.8500), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.8500), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.08047808706760406\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5368), 'map_50': tensor(0.9421), 'map_75': tensor(0.5848), 'map_small': tensor(0.6683), 'map_medium': tensor(0.6776), 'map_large': tensor(0.5077), 'mar_1': tensor(0.4246), 'mar_10': tensor(0.6246), 'mar_100': tensor(0.6246), 'mar_small': tensor(0.7000), 'mar_medium': tensor(0.7250), 'mar_large': tensor(0.5973), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.08047808706760406\n","Epoch: 8/50\n","Training Metrics:\n","{'map': tensor(0.8832), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(0.8777), 'mar_1': tensor(0.9000), 'mar_10': tensor(0.9000), 'mar_100': tensor(0.9000), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.8857), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.056826528161764145\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5485), 'map_50': tensor(0.9292), 'map_75': tensor(0.5365), 'map_small': tensor(0.7354), 'map_medium': tensor(0.6567), 'map_large': tensor(0.5212), 'mar_1': tensor(0.4218), 'mar_10': tensor(0.6408), 'mar_100': tensor(0.6408), 'mar_small': tensor(0.7625), 'mar_medium': tensor(0.7208), 'mar_large': tensor(0.6145), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.056826528161764145\n","Epoch: 9/50\n","Training Metrics:\n","{'map': tensor(0.8766), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.8766), 'mar_1': tensor(0.9000), 'mar_10': tensor(0.9000), 'mar_100': tensor(0.9000), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9000), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.03861582279205322\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5230), 'map_50': tensor(0.9418), 'map_75': tensor(0.5334), 'map_small': tensor(0.7073), 'map_medium': tensor(0.6446), 'map_large': tensor(0.4922), 'mar_1': tensor(0.4225), 'mar_10': tensor(0.6211), 'mar_100': tensor(0.6211), 'mar_small': tensor(0.7250), 'mar_medium': tensor(0.6917), 'mar_large': tensor(0.5982), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.03861582279205322\n","Epoch: 10/50\n","Training Metrics:\n","{'map': tensor(0.9165), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(0.9381), 'map_medium': tensor(0.8889), 'map_large': tensor(0.9442), 'mar_1': tensor(0.4750), 'mar_10': tensor(0.9438), 'mar_100': tensor(0.9438), 'mar_small': tensor(0.9500), 'mar_medium': tensor(0.9167), 'mar_large': tensor(0.9667), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.04688464477658272\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5487), 'map_50': tensor(0.9607), 'map_75': tensor(0.5305), 'map_small': tensor(0.7741), 'map_medium': tensor(0.6614), 'map_large': tensor(0.5225), 'mar_1': tensor(0.4338), 'mar_10': tensor(0.6514), 'mar_100': tensor(0.6514), 'mar_small': tensor(0.7875), 'mar_medium': tensor(0.7375), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.04688464477658272\n","Epoch: 11/50\n","Training Metrics:\n","{'map': tensor(0.9124), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9124), 'mar_1': tensor(0.6333), 'mar_10': tensor(0.9250), 'mar_100': tensor(0.9250), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9250), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.03288612514734268\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5436), 'map_50': tensor(0.9482), 'map_75': tensor(0.5372), 'map_small': tensor(0.7933), 'map_medium': tensor(0.6670), 'map_large': tensor(0.5127), 'mar_1': tensor(0.4317), 'mar_10': tensor(0.6479), 'mar_100': tensor(0.6479), 'mar_small': tensor(0.8125), 'mar_medium': tensor(0.7333), 'mar_large': tensor(0.6173), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.03288612514734268\n","Epoch: 12/50\n","Training Metrics:\n","{'map': tensor(0.9527), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(0.9381), 'map_medium': tensor(0.9442), 'map_large': tensor(0.9675), 'mar_1': tensor(0.6500), 'mar_10': tensor(0.9667), 'mar_100': tensor(0.9667), 'mar_small': tensor(0.9500), 'mar_medium': tensor(0.9667), 'mar_large': tensor(0.9800), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.03383747115731239\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5459), 'map_50': tensor(0.9481), 'map_75': tensor(0.5397), 'map_small': tensor(0.7953), 'map_medium': tensor(0.6855), 'map_large': tensor(0.5118), 'mar_1': tensor(0.4324), 'mar_10': tensor(0.6493), 'mar_100': tensor(0.6493), 'mar_small': tensor(0.8125), 'mar_medium': tensor(0.7417), 'mar_large': tensor(0.6173), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.03383747115731239\n","Epoch: 13/50\n","Training Metrics:\n","{'map': tensor(0.9587), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(0.9663), 'map_large': tensor(0.9565), 'mar_1': tensor(0.6500), 'mar_10': tensor(0.9667), 'mar_100': tensor(0.9667), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.9667), 'mar_large': tensor(0.9667), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.037530913949012756\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5522), 'map_50': tensor(0.9606), 'map_75': tensor(0.4942), 'map_small': tensor(0.8048), 'map_medium': tensor(0.6880), 'map_large': tensor(0.5184), 'mar_1': tensor(0.4366), 'mar_10': tensor(0.6535), 'mar_100': tensor(0.6535), 'mar_small': tensor(0.8125), 'mar_medium': tensor(0.7417), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.037530913949012756\n","Epoch: 14/50\n","Training Metrics:\n","{'map': tensor(0.9675), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9675), 'mar_1': tensor(0.7800), 'mar_10': tensor(0.9800), 'mar_100': tensor(0.9800), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9800), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.031316086649894714\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5460), 'map_50': tensor(0.9498), 'map_75': tensor(0.5523), 'map_small': tensor(0.7933), 'map_medium': tensor(0.6661), 'map_large': tensor(0.5134), 'mar_1': tensor(0.4324), 'mar_10': tensor(0.6486), 'mar_100': tensor(0.6486), 'mar_small': tensor(0.8125), 'mar_medium': tensor(0.7333), 'mar_large': tensor(0.6182), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.031316086649894714\n","Epoch: 15/50\n","Training Metrics:\n","{'map': tensor(0.9672), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(0.9000), 'map_medium': tensor(0.9505), 'map_large': tensor(0.9871), 'mar_1': tensor(0.7091), 'mar_10': tensor(0.9727), 'mar_100': tensor(0.9727), 'mar_small': tensor(0.9000), 'mar_medium': tensor(0.9500), 'mar_large': tensor(0.9875), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.02967379242181778\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5441), 'map_50': tensor(0.9488), 'map_75': tensor(0.5238), 'map_small': tensor(0.8112), 'map_medium': tensor(0.6732), 'map_large': tensor(0.5099), 'mar_1': tensor(0.4338), 'mar_10': tensor(0.6507), 'mar_100': tensor(0.6507), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7333), 'mar_large': tensor(0.6200), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.02967379242181778\n","Epoch: 16/50\n","Training Metrics:\n","{'map': tensor(0.9686), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(0.9691), 'map_medium': tensor(0.9112), 'map_large': tensor(0.9845), 'mar_1': tensor(0.4875), 'mar_10': tensor(0.9750), 'mar_100': tensor(0.9750), 'mar_small': tensor(0.9750), 'mar_medium': tensor(0.9333), 'mar_large': tensor(0.9889), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.039353322237730026\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5515), 'map_50': tensor(0.9481), 'map_75': tensor(0.5692), 'map_small': tensor(0.7921), 'map_medium': tensor(0.7135), 'map_large': tensor(0.5142), 'mar_1': tensor(0.4387), 'mar_10': tensor(0.6556), 'mar_100': tensor(0.6556), 'mar_small': tensor(0.8125), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.039353322237730026\n","Epoch: 17/50\n","Training Metrics:\n","{'map': tensor(0.9811), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9752), 'mar_1': tensor(0.6077), 'mar_10': tensor(0.9846), 'mar_100': tensor(0.9846), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9800), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.03464208543300629\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5397), 'map_50': tensor(0.9340), 'map_75': tensor(0.5658), 'map_small': tensor(0.8177), 'map_medium': tensor(0.6891), 'map_large': tensor(0.5039), 'mar_1': tensor(0.4359), 'mar_10': tensor(0.6521), 'mar_100': tensor(0.6521), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6191), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.03464208543300629\n","Epoch: 18/50\n","Training Metrics:\n","{'map': tensor(0.9880), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9840), 'mar_1': tensor(0.6154), 'mar_10': tensor(0.9923), 'mar_100': tensor(0.9923), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9875), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.034482017159461975\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5519), 'map_50': tensor(0.9490), 'map_75': tensor(0.5826), 'map_small': tensor(0.8089), 'map_medium': tensor(0.6840), 'map_large': tensor(0.5189), 'mar_1': tensor(0.4423), 'mar_10': tensor(0.6570), 'mar_100': tensor(0.6570), 'mar_small': tensor(0.8125), 'mar_medium': tensor(0.7375), 'mar_large': tensor(0.6282), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.034482017159461975\n","Epoch: 19/50\n","Training Metrics:\n","{'map': tensor(0.9686), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9588), 'mar_1': tensor(0.8667), 'mar_10': tensor(0.9778), 'mar_100': tensor(0.9778), 'mar_small': tensor(1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9714), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.02612588182091713\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5450), 'map_50': tensor(0.9399), 'map_75': tensor(0.5366), 'map_small': tensor(0.7869), 'map_medium': tensor(0.6925), 'map_large': tensor(0.5089), 'mar_1': tensor(0.4359), 'mar_10': tensor(0.6521), 'mar_100': tensor(0.6521), 'mar_small': tensor(0.8000), 'mar_medium': tensor(0.7500), 'mar_large': tensor(0.6200), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.02612588182091713\n","Epoch: 20/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(1.), 'mar_1': tensor(1.), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.015353744849562645\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5425), 'map_50': tensor(0.9402), 'map_75': tensor(0.5606), 'map_small': tensor(0.8157), 'map_medium': tensor(0.6909), 'map_large': tensor(0.5060), 'mar_1': tensor(0.4366), 'mar_10': tensor(0.6542), 'mar_100': tensor(0.6542), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6218), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.015353744849562645\n","Epoch: 21/50\n","Training Metrics:\n","{'map': tensor(0.9409), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9409), 'mar_1': tensor(0.8556), 'mar_10': tensor(0.9667), 'mar_100': tensor(0.9667), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9667), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.023400193080306053\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5490), 'map_50': tensor(0.9483), 'map_75': tensor(0.5660), 'map_small': tensor(0.8112), 'map_medium': tensor(0.6962), 'map_large': tensor(0.5129), 'mar_1': tensor(0.4401), 'mar_10': tensor(0.6585), 'mar_100': tensor(0.6585), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6273), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.023400193080306053\n","Epoch: 22/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(1.), 'map_medium': tensor(-1.), 'map_large': tensor(1.), 'mar_1': tensor(0.7273), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.023317310959100723\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5487), 'map_50': tensor(0.9410), 'map_75': tensor(0.5537), 'map_small': tensor(0.8071), 'map_medium': tensor(0.6953), 'map_large': tensor(0.5136), 'mar_1': tensor(0.4408), 'mar_10': tensor(0.6577), 'mar_100': tensor(0.6577), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6264), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.023317310959100723\n","Epoch: 23/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.5714), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.023448454216122627\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5460), 'map_50': tensor(0.9407), 'map_75': tensor(0.5623), 'map_small': tensor(0.8071), 'map_medium': tensor(0.6984), 'map_large': tensor(0.5090), 'mar_1': tensor(0.4394), 'mar_10': tensor(0.6563), 'mar_100': tensor(0.6563), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7500), 'mar_large': tensor(0.6236), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.023448454216122627\n","Epoch: 24/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.6154), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.023672711104154587\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5445), 'map_50': tensor(0.9407), 'map_75': tensor(0.5552), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7015), 'map_large': tensor(0.5072), 'mar_1': tensor(0.4394), 'mar_10': tensor(0.6563), 'mar_100': tensor(0.6563), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.023672711104154587\n","Epoch: 25/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.8000), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.018034838140010834\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5468), 'map_50': tensor(0.9406), 'map_75': tensor(0.5622), 'map_small': tensor(0.8071), 'map_medium': tensor(0.6990), 'map_large': tensor(0.5101), 'mar_1': tensor(0.4394), 'mar_10': tensor(0.6563), 'mar_100': tensor(0.6563), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.018034838140010834\n","Epoch: 26/50\n","Training Metrics:\n","{'map': tensor(0.9917), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9917), 'mar_1': tensor(0.5333), 'mar_10': tensor(0.9933), 'mar_100': tensor(0.9933), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9933), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.026651473715901375\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5450), 'map_50': tensor(0.9406), 'map_75': tensor(0.5466), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7015), 'map_large': tensor(0.5077), 'mar_1': tensor(0.4394), 'mar_10': tensor(0.6563), 'mar_100': tensor(0.6563), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7500), 'mar_large': tensor(0.6236), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.026651473715901375\n","Epoch: 27/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.8000), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.02075350098311901\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5434), 'map_50': tensor(0.9407), 'map_75': tensor(0.5611), 'map_small': tensor(0.8071), 'map_medium': tensor(0.6976), 'map_large': tensor(0.5080), 'mar_1': tensor(0.4387), 'mar_10': tensor(0.6542), 'mar_100': tensor(0.6542), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6218), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.02075350098311901\n","Epoch: 28/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.7273), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.026835991069674492\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5461), 'map_50': tensor(0.9402), 'map_75': tensor(0.5814), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7093), 'map_large': tensor(0.5088), 'mar_1': tensor(0.4401), 'mar_10': tensor(0.6563), 'mar_100': tensor(0.6563), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.026835991069674492\n","Epoch: 29/50\n","Training Metrics:\n","{'map': tensor(0.9904), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9857), 'mar_1': tensor(0.4938), 'mar_10': tensor(0.9937), 'mar_100': tensor(0.9937), 'mar_small': tensor(1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9889), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.027273420244455338\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5456), 'map_50': tensor(0.9411), 'map_75': tensor(0.5549), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7000), 'map_large': tensor(0.5080), 'mar_1': tensor(0.4387), 'mar_10': tensor(0.6549), 'mar_100': tensor(0.6549), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7458), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.027273420244455338\n","Epoch: 30/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.6667), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.018739506602287292\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5484), 'map_50': tensor(0.9533), 'map_75': tensor(0.5644), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7091), 'map_large': tensor(0.5100), 'mar_1': tensor(0.4423), 'mar_10': tensor(0.6585), 'mar_100': tensor(0.6585), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6255), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.018739506602287292\n","Epoch: 31/50\n","Training Metrics:\n","{'map': tensor(0.9845), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9845), 'mar_1': tensor(0.8778), 'mar_10': tensor(0.9889), 'mar_100': tensor(0.9889), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9889), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.021175362169742584\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5473), 'map_50': tensor(0.9407), 'map_75': tensor(0.5623), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7014), 'map_large': tensor(0.5106), 'mar_1': tensor(0.4408), 'mar_10': tensor(0.6570), 'mar_100': tensor(0.6570), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7500), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.021175362169742584\n","Epoch: 32/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.5333), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.023347601294517517\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5469), 'map_50': tensor(0.9409), 'map_75': tensor(0.5623), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7075), 'map_large': tensor(0.5095), 'mar_1': tensor(0.4408), 'mar_10': tensor(0.6570), 'mar_100': tensor(0.6570), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6236), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.023347601294517517\n","Epoch: 33/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.6667), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.021507184952497482\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5517), 'map_50': tensor(0.9410), 'map_75': tensor(0.5860), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7159), 'map_large': tensor(0.5132), 'mar_1': tensor(0.4444), 'mar_10': tensor(0.6613), 'mar_100': tensor(0.6613), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6282), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.021507184952497482\n","Epoch: 34/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(1.), 'mar_1': tensor(1.), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.013790513388812542\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5452), 'map_50': tensor(0.9407), 'map_75': tensor(0.5684), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7031), 'map_large': tensor(0.5084), 'mar_1': tensor(0.4401), 'mar_10': tensor(0.6570), 'mar_100': tensor(0.6570), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7500), 'mar_large': tensor(0.6236), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.013790513388812542\n","Epoch: 35/50\n","Training Metrics:\n","{'map': tensor(0.9926), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9901), 'mar_1': tensor(0.5333), 'mar_10': tensor(0.9933), 'mar_100': tensor(0.9933), 'mar_small': tensor(1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9900), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.021023930981755257\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5436), 'map_50': tensor(0.9409), 'map_75': tensor(0.5625), 'map_small': tensor(0.8071), 'map_medium': tensor(0.7099), 'map_large': tensor(0.5066), 'mar_1': tensor(0.4387), 'mar_10': tensor(0.6549), 'mar_100': tensor(0.6549), 'mar_small': tensor(0.8250), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6209), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.021023930981755257\n","Epoch: 36/50\n","Training Metrics:\n","{'map': tensor(0.9869), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9869), 'mar_1': tensor(0.8778), 'mar_10': tensor(0.9889), 'mar_100': tensor(0.9889), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9889), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.013565467670559883\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5518), 'map_50': tensor(0.9408), 'map_75': tensor(0.5846), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7042), 'map_large': tensor(0.5139), 'mar_1': tensor(0.4437), 'mar_10': tensor(0.6606), 'mar_100': tensor(0.6606), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7500), 'mar_large': tensor(0.6282), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.013565467670559883\n","Epoch: 37/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.7273), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.016758108511567116\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5505), 'map_50': tensor(0.9407), 'map_75': tensor(0.5776), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7088), 'map_large': tensor(0.5132), 'mar_1': tensor(0.4415), 'mar_10': tensor(0.6599), 'mar_100': tensor(0.6599), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6264), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.016758108511567116\n","Epoch: 38/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.8000), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.019229834899306297\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5510), 'map_50': tensor(0.9407), 'map_75': tensor(0.5835), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7148), 'map_large': tensor(0.5120), 'mar_1': tensor(0.4423), 'mar_10': tensor(0.6592), 'mar_100': tensor(0.6592), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.019229834899306297\n","Epoch: 39/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.7273), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.022156430408358574\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5508), 'map_50': tensor(0.9408), 'map_75': tensor(0.5697), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7116), 'map_large': tensor(0.5136), 'mar_1': tensor(0.4423), 'mar_10': tensor(0.6599), 'mar_100': tensor(0.6599), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6264), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.022156430408358574\n","Epoch: 40/50\n","Training Metrics:\n","{'map': tensor(0.9871), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.9871), 'mar_1': tensor(0.9875), 'mar_10': tensor(0.9875), 'mar_100': tensor(0.9875), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.9875), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.015242740511894226\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5507), 'map_50': tensor(0.9407), 'map_75': tensor(0.5846), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7159), 'map_large': tensor(0.5122), 'mar_1': tensor(0.4415), 'mar_10': tensor(0.6592), 'mar_100': tensor(0.6592), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.015242740511894226\n","Epoch: 41/50\n","Training Metrics:\n","{'map': tensor(0.9904), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9885), 'mar_1': tensor(0.4938), 'mar_10': tensor(0.9937), 'mar_100': tensor(0.9937), 'mar_small': tensor(1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9909), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.021695295348763466\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5493), 'map_50': tensor(0.9408), 'map_75': tensor(0.5777), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7176), 'map_large': tensor(0.5105), 'mar_1': tensor(0.4408), 'mar_10': tensor(0.6592), 'mar_100': tensor(0.6592), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.021695295348763466\n","Epoch: 42/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.5333), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.026027316227555275\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5512), 'map_50': tensor(0.9407), 'map_75': tensor(0.5847), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7159), 'map_large': tensor(0.5126), 'mar_1': tensor(0.4423), 'mar_10': tensor(0.6599), 'mar_100': tensor(0.6599), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6255), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.026027316227555275\n","Epoch: 43/50\n","Training Metrics:\n","{'map': tensor(0.9871), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(0.9851), 'mar_1': tensor(0.9875), 'mar_10': tensor(0.9875), 'mar_100': tensor(0.9875), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(0.9857), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.014877954497933388\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5453), 'map_50': tensor(0.9409), 'map_75': tensor(0.5697), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7100), 'map_large': tensor(0.5084), 'mar_1': tensor(0.4394), 'mar_10': tensor(0.6570), 'mar_100': tensor(0.6570), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.014877954497933388\n","Epoch: 44/50\n","Training Metrics:\n","{'map': tensor(0.9901), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(0.9825), 'map_large': tensor(1.), 'mar_1': tensor(0.5714), 'mar_10': tensor(0.9929), 'mar_100': tensor(0.9929), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.9875), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.024486331269145012\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5500), 'map_50': tensor(0.9408), 'map_75': tensor(0.5846), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7159), 'map_large': tensor(0.5118), 'mar_1': tensor(0.4415), 'mar_10': tensor(0.6592), 'mar_100': tensor(0.6592), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.024486331269145012\n","Epoch: 45/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.6667), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.02123459242284298\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5487), 'map_50': tensor(0.9409), 'map_75': tensor(0.5697), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7168), 'map_large': tensor(0.5104), 'mar_1': tensor(0.4415), 'mar_10': tensor(0.6592), 'mar_100': tensor(0.6592), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7583), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.02123459242284298\n","Epoch: 46/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.5714), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.02231418527662754\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5461), 'map_50': tensor(0.9409), 'map_75': tensor(0.5686), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7116), 'map_large': tensor(0.5090), 'mar_1': tensor(0.4394), 'mar_10': tensor(0.6570), 'mar_100': tensor(0.6570), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6227), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.02231418527662754\n","Epoch: 47/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.8000), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.017570775002241135\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5476), 'map_50': tensor(0.9409), 'map_75': tensor(0.5540), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7108), 'map_large': tensor(0.5105), 'mar_1': tensor(0.4408), 'mar_10': tensor(0.6585), 'mar_100': tensor(0.6585), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.017570775002241135\n","Epoch: 48/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(1.), 'map_large': tensor(1.), 'mar_1': tensor(0.8000), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.016271643340587616\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5473), 'map_50': tensor(0.9407), 'map_75': tensor(0.5674), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7099), 'map_large': tensor(0.5102), 'mar_1': tensor(0.4401), 'mar_10': tensor(0.6577), 'mar_100': tensor(0.6577), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6236), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.016271643340587616\n","Epoch: 49/50\n","Training Metrics:\n","{'map': tensor(1.), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(1.), 'mar_1': tensor(1.), 'mar_10': tensor(1.), 'mar_100': tensor(1.), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/100, Loss: 0.012291057966649532\n","Validation on Validation Set:\n","\n","{'map': tensor(0.5485), 'map_50': tensor(0.9408), 'map_75': tensor(0.5687), 'map_small': tensor(0.8215), 'map_medium': tensor(0.7100), 'map_large': tensor(0.5118), 'mar_1': tensor(0.4408), 'mar_10': tensor(0.6585), 'mar_100': tensor(0.6585), 'mar_small': tensor(0.8375), 'mar_medium': tensor(0.7542), 'mar_large': tensor(0.6245), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","Iteration: 100/13, Val Loss: 0.012291057966649532\n"]}],"source":["import torch\n","import torchvision\n","#import config\n","#from utils import (\n","    #get_model_instance_segmentation,\n","    #ollate_fn,\n","    #get_transform,\n","    #myOwnDataset,\n","#)\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from torch.optim.lr_scheduler import StepLR\n","\n","\n","print(\"Torch version:\", torch.__version__)\n","\n","num_epochs = 50\n","\n","train_data_dir = '/content/drive/MyDrive/niaochao COCO/train'\n","train_coco = '/content/drive/MyDrive/niaochao COCO/train/_annotations.coco.json'\n","\n","validation_data_dir = '/content/drive/MyDrive/niaochao COCO/valid'\n","validation_coco = '/content/drive/MyDrive/niaochao COCO/valid/_annotations.coco.json'\n","\n","# Training dataset\n","my_dataset = myOwnDataset(\n","    root=train_data_dir, annotation=train_coco, transforms=get_transform()\n",")\n","\n","# own DataLoader for training dataset\n","data_loader = torch.utils.data.DataLoader(\n","    my_dataset,\n","    batch_size=8,\n","    shuffle=True,\n","    num_workers=2,#work around for windows way of handling number of workers\n","    collate_fn=collate_fn,\n",")\n","\n","# Validation dataset\n","validation_dataset = myOwnDataset(\n","    root=validation_data_dir, annotation=validation_coco, transforms=get_transform()\n",")\n","\n","# own DataLoader for validation dataset\n","validation_data_loader = torch.utils.data.DataLoader(\n","    validation_dataset,\n","    batch_size=8,\n","    shuffle=True,\n","    num_workers=2,#work around for windows way of handling number of workers\n","    collate_fn=collate_fn,\n",")\n","\n","# select device (whether GPU or CPU)\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","# DataLoader is iterable over Dataset\n","for imgs, annotations in data_loader:\n","    imgs = list(img.to(device) for img in imgs)\n","    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","    #print(annotations)\n","\n","# DataLoader is iterable over Dataset\n","for vimgs, vannotations in validation_data_loader:\n","    vimgs = list(img.to(device) for img in vimgs)\n","    vannotations = [{k: v.to(device) for k, v in t.items()} for t in vannotations]\n","\n","model = get_model_instance_segmentation(2)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# parameters\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.002,weight_decay=0.0001, momentum=0.9, nesterov=True)\n","scheduler = StepLR(optimizer=optimizer, step_size=10, gamma=0.5,verbose=True)\n","\n","len_dataloader = len(data_loader)\n","len_vdataloader = len(validation_data_loader)\n","\n","metric_map = MeanAveragePrecision()\n","metric_mapVal = MeanAveragePrecision()\n","accuracy = 0.0\n","\n","\n","print(\"Model Structure:\")\n","print(model)\n","# Training\n","for epoch in range(num_epochs):\n","    print(f\"Epoch: {epoch}/{num_epochs}\")\n","    model.train()\n","    i = 0\n","    metric_map.reset()\n","    for imgs, annotations in data_loader:\n","        i += 1\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        loss_dict = model(imgs, annotations)#returns multiple different types of losses from the faster RCNN model\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # Extract individual loss values\n","        loss_classifier = loss_dict['loss_classifier'].item()\n","        loss_box_reg = loss_dict['loss_box_reg'].item()\n","        loss_objectness = loss_dict['loss_objectness'].item()\n","        loss_rpn_box_reg = loss_dict['loss_rpn_box_reg'].item()\n","\n","        # Display each loss from the loss_dict\n","        #for loss_name, loss_value in loss_dict.items():\n","          #print(f\"{loss_name}: {loss_value.item()}\")\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    model.eval()\n","    with torch.no_grad():\n","      predictions = model(imgs)\n","      metric_map.update(predictions, annotations)\n","\n","    map_metric = metric_map.compute()\n","    print(\"Training Metrics:\")\n","    print(map_metric)\n","\n","    print(f\"Iteration: {i}/{len_dataloader}, Loss: {losses}\")\n","    #validation after each epoch\n","\n","    #mAP metrics\n","    #metric_map.reset()\n","\n","##########################################################################################################################\n","\n","    print(\"Validation on Validation Set:\\n\")\n","    model.eval()\n","    metric_mapVal.reset()\n","    with torch.no_grad():\n","      for vimgs, vannotations in validation_data_loader:\n","        vimgs = list(img.to(device) for img in vimgs)\n","        vannotations = [{k: v.to(device) for k, v in t.items()} for t in vannotations]\n","\n","        model.train()\n","        vloss_dict = model(vimgs, vannotations)\n","        vlosses = sum(loss for loss in loss_dict.values())\n","\n","        # Extract individual loss values\n","        vloss_classifier = vloss_dict['loss_classifier'].item()\n","        vloss_box_reg = vloss_dict['loss_box_reg'].item()\n","        vloss_objectness = vloss_dict['loss_objectness'].item()\n","        vloss_rpn_box_reg = vloss_dict['loss_rpn_box_reg'].item()\n","        model.eval()\n","\n","        predictionsv = model(vimgs)\n","        metric_mapVal.update(predictionsv, vannotations)\n","\n","      map_metricVal = metric_mapVal.compute()\n","      print(map_metricVal)\n","\n","      if(map_metricVal['map'].item()>accuracy):\n","        torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNBestWeights.pth')\n","        torch.save(model,'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNModelBestWeights.pt')\n","        accuracy = map_metricVal['map'].item()\n","\n","      if(epoch ==  9):\n","        torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch10.pth')\n","        torch.save(model,'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch10.pt')\n","\n","      if(epoch ==  19):\n","        torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch20.pth')\n","        torch.save(model,'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch20.pt')\n","\n","      if(epoch ==  29):\n","        torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch30.pth')\n","        torch.save(model,'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch30.pt')\n","\n","      if(epoch ==  39):\n","        torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch40.pth')\n","        torch.save(model,'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch40.pt')\n","\n","      if(epoch ==  49):\n","        torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch50.pth')\n","        torch.save(model,'/content/drive/MyDrive/FasterRCNNFiles/FasterRCNNEpoch50.pt')\n","\n","\n","      # Save the losses to a CSV file\n","      with open(csv_file, mode='a', newline='') as file:\n","          writer = csv.writer(file)\n","          writer.writerow([epoch, i, loss_classifier, loss_box_reg, loss_objectness, loss_rpn_box_reg, losses, vloss_classifier, vloss_box_reg,vloss_objectness,vloss_rpn_box_reg, vlosses,\n","                         map_metric['map'].item(), map_metric['map_50'].item(), map_metric['map_75'].item(), map_metric['mar_1'].item(), map_metric['mar_10'].item(), map_metric['mar_100'].item(),\n","                         map_metric['map_small'].item(), map_metric['map_medium'].item(), map_metric['map_large'].item(), map_metric['mar_small'].item(), map_metric['mar_medium'].item(), map_metric['mar_large'].item(),\n","                         map_metricVal['map'].item(), map_metricVal['map_50'].item(), map_metricVal['map_75'].item(), map_metricVal['mar_1'].item(), map_metricVal['mar_10'].item(), map_metricVal['mar_100'].item(),\n","                         map_metricVal['map_small'].item(), map_metricVal['map_medium'].item(), map_metricVal['map_large'].item(), map_metricVal['mar_small'].item(), map_metricVal['mar_medium'].item(), map_metricVal['mar_large'].item(),\n","                         lr])\n","\n","      print(f\"Iteration: {i}/{len_vdataloader}, Val Loss: {vlosses}\")\n","\n","      scheduler.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_56Jpt6mb9r"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":475,"status":"ok","timestamp":1727251611237,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"O_Qm3Ph8K1Wv","outputId":"a2625d76-c56f-41e4-8419-094bb02af508"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAB2Mb7RF9sr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw-qCVrTK1Oz"},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision import transforms as T\n","\n","from PIL import Image#reading images\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","\n","ig = Image.open('/content/drive/MyDrive/niaochao COCO/test/100042_jpg.rf.28a618c076bfe7a8e2f6c1f0cb038904.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1727251620890,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"Csrx-zYpLMj1","outputId":"8e5f33e1-4f10-4a2f-de1f-8522694eebb3"},"outputs":[{"data":{"text/plain":["[{'boxes': tensor([[ 572.9694, 1382.7177, 1568.3412, 2079.7786],\n","          [ 438.5985, 1013.0834, 1743.2255, 2308.6733]], device='cuda:0'),\n","  'labels': tensor([1, 1], device='cuda:0'),\n","  'scores': tensor([0.9952, 0.1038], device='cuda:0')}]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\n","transformqw = T.Compose([T.ToTensor()])\n","img = transformqw(ig)\n","img = img.to(device)#fixed the issue\n","\n","with torch.no_grad():\n","  pred = model([img])\n","\n","pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgP4DzHzaeWT"},"outputs":[],"source":["bboxes, labels, scores = pred[0][\"boxes\"], pred[0][\"labels\"], pred[0][\"scores\"]\n","num = torch.argwhere(scores > 0.7).shape[0]#get where scores greater than 90%\n","num = 1\n","bboxes = bboxes.cpu()\n","igg=cv2.imread('/content/drive/MyDrive/niaochao COCO/test/100042_jpg.rf.28a618c076bfe7a8e2f6c1f0cb038904.jpg')\n","font=cv2.FONT_HERSHEY_SIMPLEX\n","for i in range(num):\n","  x1, y1, x2, y2 = bboxes[i].numpy().astype(\"int\")\n","  #print(x1, y1, x2, y2)\n","  #class_name = \"nest\"#mapping to class names\n","  igg = cv2.rectangle(igg,(x1,y1),(x2,y2),(0,255,0),1)\n","  #igg = cv2.putText(igg, class_name, (x1,y1-10),font,0.5,(255,0,0),1,cv2.LINE_AA)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":934,"output_embedded_package_id":"1IhSLDpCNCUJq-xD_yaPYWD9xaeipYfyo"},"executionInfo":{"elapsed":10627,"status":"ok","timestamp":1727251643200,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"lRIo4-J3fD_h","outputId":"b38a7fd4-33be-41d7-c685-00dc78f46aa9"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["cv2_imshow(igg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klKOuedefGkg"},"outputs":[],"source":["torch.save(model.state_dict(),'/content/drive/MyDrive/FasterRCNNEpoch10ModelPretrainedBatchSize4.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSIjhVVHgJSL"},"outputs":[],"source":["torch.save(model,'/content/drive/MyDrive/FasterRCNNEpoch10ModelPretrainedBatchSize4.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1728748867713,"user":{"displayName":"Thomas Koen","userId":"05648008293042431593"},"user_tz":-120},"id":"yzIsKXb2OnVY","outputId":"ca04159a-84c3-424c-f7e2-43fb047658bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(640,), max_size=640, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["#to see model structure\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcW3VXiwdkkf"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMJDQdZ+7xdBJRihhgzcv2j","gpuType":"T4","provenance":[{"file_id":"1EstOIvmyCLJlXqbj4QWj2hGihzr5ZUN8","timestamp":1726142959685}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
